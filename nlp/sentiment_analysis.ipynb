{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python_dl_nlp",
   "display_name": "Python (DL/NLP)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\Rob\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Rob\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_text = '''Like you, I am getting very frustrated with this process. I am genuinely trying to be as reasonable as possible. I am not trying to \"hold up\" the deal at the last minute. I'm afraid that I am being asked to take a fairly large leap of faith after this company (I don't mean the two of you -- I mean Enron) has screwed me and the people who work for me.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'neg': 0.093, 'neu': 0.836, 'pos': 0.071, 'compound': -0.3804}"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = sid.polarity_scores(message_text)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "STOP_WORDS = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Like',\n 'you,',\n 'I',\n 'am',\n 'getting',\n 'very',\n 'frustrated',\n 'with',\n 'this',\n 'process.',\n 'I',\n 'am',\n 'genuinely',\n 'trying',\n 'to',\n 'be',\n 'as',\n 'reasonable',\n 'as',\n 'possible.',\n 'I',\n 'am',\n 'not',\n 'trying',\n 'to',\n '\"hold',\n 'up\"',\n 'the',\n 'deal',\n 'at',\n 'the',\n 'last',\n 'minute.',\n \"I'm\",\n 'afraid',\n 'that',\n 'I',\n 'am',\n 'being',\n 'asked',\n 'to',\n 'take',\n 'a',\n 'fairly',\n 'large',\n 'leap',\n 'of',\n 'faith',\n 'after',\n 'this',\n 'company',\n '(I',\n \"don't\",\n 'mean',\n 'the',\n 'two',\n 'of',\n 'you',\n '--',\n 'I',\n 'mean',\n 'Enron)',\n 'has',\n 'screwed',\n 'me',\n 'and',\n 'the',\n 'people',\n 'who',\n 'work',\n 'for',\n 'me.']"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = message_text.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "df = pd.DataFrame(message_text.split(), columns=['words'])\n",
    "\n",
    "for doc in tokenizer.pipe(df['words']):\n",
    "    \n",
    "    doc_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.is_stop == False) & (token.is_punct == False):\n",
    "            doc_tokens.append(token.text.lower())\n",
    "\n",
    "    tokens.append(doc_tokens)\n",
    "\n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'like you, getting frustrated process. genuinely trying reasonable possible. trying \"hold up\" deal minute. i\\'m afraid asked fairly large leap faith company (i don\\'t mean mean enron) screwed people work me.'"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = sum(list([item for item in df['tokens'] if len(item) != 0]), [])\n",
    "message = ' '.join(word_list)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'neg': 0.17, 'neu': 0.694, 'pos': 0.136, 'compound': -0.3182}"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = sid.polarity_scores(message)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = '''\n",
    "I never thought you were a saint\n",
    "I just wanted you to be the one\n",
    "I always figure you would stay\n",
    "But now you’re gone\n",
    "You’re gone\n",
    "\n",
    "So burn away my time\n",
    "I want your hand in mine\n",
    "It hurt to see you go\n",
    "'cause I was in love with you\n",
    "\n",
    "I’m caught here balanced on a wire\n",
    "Bound between your heart and mine\n",
    "I should have given so much more\n",
    "Instead I watched you walk away\n",
    "\n",
    "In my mind\n",
    "There you are\n",
    "With me\n",
    "In your arms\n",
    "I feel so alive\n",
    "When you're mine\n",
    "But here we are\n",
    "You’re gone\n",
    "On your own\n",
    "I’m here\n",
    "All alone\n",
    "I just could not speak\n",
    "A word\n",
    "So within my mind\n",
    "Here you are\n",
    "With me\n",
    "In your arms\n",
    "I feel so alive\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "STOP_WORDS = nlp.Defaults.stop_words\n",
    "\n",
    "import pandas as pd \n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_score(data=lyrics):\n",
    "    tokens = []\n",
    "    df = pd.DataFrame(data.split(), columns=['words'])\n",
    "\n",
    "    for doc in tokenizer.pipe(df['words']):\n",
    "        doc_tokens = []\n",
    "        for token in doc:\n",
    "            if (token.is_stop == False) & (token.is_punct == False):\n",
    "                doc_tokens.append(token.text.lower())\n",
    "        tokens.append(doc_tokens)\n",
    "\n",
    "    df['tokens'] = tokens\n",
    "    word_list = sum(list([item for item in df['tokens'] if len(item) != 0]), [])\n",
    "    lyrics_processed = ' '.join(word_list)\n",
    "    scores = sid.polarity_scores(lyrics_processed)\n",
    "\n",
    "    return scores['compound']\n",
    "\n",
    "def stemmed_score(data=lyrics):\n",
    "    \"\"\" Processes the text via spacy \"\"\"\n",
    "    tokens = []\n",
    "    words = []\n",
    "\n",
    "    # Stemming\n",
    "    for word in data.split():\n",
    "        words.append(ps.stem(word))\n",
    "\n",
    "    df = pd.DataFrame(words, columns=['words'])\n",
    "\n",
    "    for doc in tokenizer.pipe(df['words']):\n",
    "        doc_tokens = []\n",
    "        for token in doc:\n",
    "            if (token.is_stop == False) & (token.is_punct == False):\n",
    "                doc_tokens.append(token.text.lower())\n",
    "        tokens.append(doc_tokens)\n",
    "\n",
    "    df['tokens'] = tokens\n",
    "    word_list = sum(list([item for item in df['tokens'] if len(item) != 0]), [])\n",
    "    lyrics_processed = ' '.join(word_list)\n",
    "    scores = sid.polarity_scores(lyrics_processed)\n",
    "\n",
    "    return scores['compound']\n",
    "\n",
    "def get_lemmas(data):\n",
    "    \"\"\" Gets lemmas for text \"\"\"\n",
    "    lemmas = []\n",
    "    doc = nlp(data)\n",
    "    \n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_!= 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "def lemma_score(data=lyrics):\n",
    "    \"\"\" Processes the text via spacy \"\"\"\n",
    "    tokens = []\n",
    "    words = []\n",
    "\n",
    "    # Lemmatization\n",
    "    words = get_lemmas(lyrics)\n",
    "\n",
    "    df = pd.DataFrame(words, columns=['words'])\n",
    "\n",
    "    for doc in tokenizer.pipe(df['words']):\n",
    "        doc_tokens = []\n",
    "        for token in doc:\n",
    "            if (token.is_stop == False) & (token.is_punct == False):\n",
    "                doc_tokens.append(token.text.lower())\n",
    "        tokens.append(doc_tokens)\n",
    "\n",
    "    df['tokens'] = tokens\n",
    "    word_list = sum(list([item for item in df['tokens'] if len(item) != 0]), [])\n",
    "    lyrics_processed = ' '.join(word_list)\n",
    "    scores = sid.polarity_scores(lyrics_processed)\n",
    "\n",
    "    return scores['compound']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(0.8591, 0.6808, 0.8689)"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_score(), stemmed_score(), lemma_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}